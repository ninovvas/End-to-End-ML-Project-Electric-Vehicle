{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15790412-3e57-4880-8f8d-5d6aa3932a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Magic Command to display plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d3621ac-d6a8-4af7-8a7a-1a29850967b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9da51dcd-98f3-4f10-a0b2-21f33353b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {'Algorithm': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': []}\n",
    "\n",
    "# Define a list of classifiers and their corresponding hyperparameters for grid search\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "param_grid = {\n",
    "    'Logistic Regression': [{'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}],\n",
    "    'K-Nearest Neighbors':  {'n_neighbors': [1, 3, 5, 7, 9]},\n",
    "    'Support Vector Machine': {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']},\n",
    "    'Decision Tree': {'max_depth': [None, 5, 10, 15, 20]},\n",
    "    'Random Forest': {'n_estimators': [10, 50, 100, 200]},\n",
    "    'Naive Bayes':  {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "278741f1-ca4f-42c7-9de8-82d063855bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
      "\n",
      "Logistic Regression\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\python39\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.64761905        nan 0.84761905        nan 0.9047619\n",
      "        nan 0.94285714        nan 0.94285714        nan 0.95238095]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Print the classification report and confusion matrix for each classifier\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "# Iterate through the classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    # Create a pipeline with scaling and the classifier\n",
    "    grid_search = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        GridSearchCV(clf, param_grid = param_grid[name], cv=5, n_jobs=-1, refit=True)\n",
    "        #('classifier', clf)\n",
    "    )\n",
    "    print( params[name])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#clf = make_pipeline(StandardScaler(), \n",
    "#                    GridSearchCV(LogisticRegression(),\n",
    "#                                 param_grid={'logisticregression__C': [0.1, 10.]},\n",
    "#                                 cv=2,\n",
    "#                                 refit=True))\n",
    "\n",
    "#clf.fit()\n",
    "#clf.predict()\n",
    "\n",
    "\n",
    "    # Use grid search to find the best hyperparameters\n",
    "    #grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid[name], cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)    # Make predictions\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results['Algorithm'].append(name)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['Precision'].append(precision)\n",
    "    results['Recall'].append(recall)\n",
    "    results['F1 Score'].append(f1_score)\n",
    "\n",
    "    # Print the classification report and confusion matrix for each classifier\n",
    "    print(f'\\n{name}\\n')\n",
    "    print(f'Best Hyperparameters: {grid_search.best_params_}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1_score:.4f}\\n')\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Plot the metrics of each algorithm\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n",
    "df_results.set_index('Algorithm')[['Accuracy', 'Precision', 'Recall', 'F1 Score']].plot(kind='bar', ax=axes.flatten(), rot=45)\n",
    "plt.suptitle('Algorithm Performance Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7da867-4031-4372-be77-c44030b7ea93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
